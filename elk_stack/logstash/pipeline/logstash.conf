input {
	udp {
		host => "0.0.0.0"
		port => 5045
	}
    beats {
        port => 5044
    }
    
    tcp {
        port => 50000
    }
    
    file {
        #https://www.elastic.co/guide/en/logstash/current/plugins-inputs-file.html
        #default is TAIL which assumes more data will come into the file.
        #change to mode => "read" if the file is a complete file.
        #by default, the file will be removed once reading is complete -- backup your files if you need them.
        # we will be using READ with the completed file action to log to a file.
        mode => "read"
        path => "/usr/share/logstash/ingest_data/*.csv" #specifying only csv files.
#        exit_after_read => true
        file_completed_action => "log" # this tells logstash to log to the file specified in file_completed_log_path once its done reading the input file.
        file_completed_log_path => "/usr/share/logstash/ingest_data/logstash_completed.log"
    }
}

filter {
  csv {
    separator => ","
    columns => [
      "Unique ID", "Indicator ID", "Name", "Measure", "Measure Info",
      "Geo Type Name", "Geo Join ID", "Geo Place Name", "Time Period",
      "Start_Date", "Data Value", "Message"
    ]
  }

  # Convertissez les champs en types de données appropriés si nécessaire
  mutate {
    convert => {
      "Unique ID" => "integer"
      "Indicator ID" => "integer"
      "Geo Join ID" => "integer"
      "Data Value" => "float"
    }
  }

  # Si vous souhaitez analyser la date
  date {
    match => ["Start_Date", "MM/dd/yyyy"]
    target => "Start_Date"
  }
}


output {
    elasticsearch {
        index => "logstash-%{+YYYY.MM.dd}"
        hosts=> ["https://es01:9200"]
        user=> "elastic"
        password=> "${ELASTIC_PASSWORD}"
        ssl_enabled => true
        cacert=> "/usr/share/logstash/certs/ca/ca.crt"
    }
}