services:
    ## Mise en place stack KAFKA
    zookeeper:
        platform: linux/amd64
        image: confluentinc/cp-zookeeper:${CONFLUENT_PLATFORM_VERSION:-7.7.0}
        container_name: zookeeper
        restart: unless-stopped
        ports:
            - '32181:32181'
            - '2888:2888'
            - '3888:3888'
        environment:
            ZOOKEEPER_SERVER_ID: 1
            ZOOKEEPER_CLIENT_PORT: 32181
            ZOOKEEPER_TICK_TIME: 2000
            ZOOKEEPER_INIT_LIMIT: 5
            ZOOKEEPER_SYNC_LIMIT: 2
            ZOOKEEPER_SERVERS: zookeeper:2888:3888
        volumes:
            - zookeeper_data:/var/lib/zookeeper/data
            - zookeeper_log:/var/lib/zookeeper/log
        healthcheck:
            test: echo stat | nc localhost 32181
            interval: 10s
            timeout: 10s
            retries: 3
        networks:
            - kafka
        logging:
            driver: "json-file"
            options:
                max-size: "255m"
        
    broker-1:
        platform: linux/amd64
        image: confluentinc/cp-kafka:${CONFLUENT_PLATFORM_VERSION:-7.7.0}
        hostname: broker-1
        container_name: broker-1
        restart: unless-stopped
        ports:
            - '19092:19092'
            - '9092:9092'
            - '9992:9992'
            - '29092:29092'
            - '39092:39092'
        depends_on:
            - zookeeper
        environment:
            KAFKA_BROKER_ID: 1
            KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
            KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT
            KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
            KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker-1:29092,EXTERNAL://localhost:9092
            #KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker-1:29092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092,DOCKER://host.docker.internal:29092
            KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
            KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
            KAFKA_DEFAULT_REPLICATION_FACTOR: 3
            KAFKA_NUM_PARTITIONS: 3
            KAFKA_JMX_PORT: 19102
            KAFKA_JMX_HOSTNAME: ${PUBLIC_IP:-127.0.0.1}
        volumes:
            - broker_1_data:/var/lib/kafka/data
        healthcheck:
            test: nc -vz localhost 9092
            interval: 10s
            timeout: 10s
            retries: 3
        networks:
            - kafka
        logging:
            driver: "json-file"
            options:
                max-size: "1m"
    
    broker-2:
        platform: linux/amd64
        image: confluentinc/cp-kafka:${CONFLUENT_PLATFORM_VERSION:-7.7.0}
        container_name: broker-2
        restart: unless-stopped
        ports:
            - '9093:9093'
            - '19093:19093'
            - '29093:29093'
            - '9993:9993'
            - '39093:39093'
        depends_on:
            - zookeeper
        environment:
            KAFKA_BROKER_ID: 2
            KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
            KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT
            KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
            KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker-2:29093,EXTERNAL://localhost:9093
            #KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker-2:29093,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9093,DOCKER://host.docker.internal:29093
            KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
            KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
            KAFKA_DEFAULT_REPLICATION_FACTOR: 3
            KAFKA_NUM_PARTITIONS: 3
            KAFKA_JMX_PORT: 19103
            KAFKA_JMX_HOSTNAME: ${PUBLIC_IP:-127.0.0.1}
        volumes:
            - broker_2_data:/var/lib/kafka/data
        healthcheck:
            test: nc -vz localhost 9093
            interval: 10s
            timeout: 10s
            retries: 3
        networks:
            - kafka
        logging:
            driver: "json-file"
            options:
                max-size: "1m"
    
    broker-3:
        platform: linux/amd64
        image: confluentinc/cp-kafka:${CONFLUENT_PLATFORM_VERSION:-7.7.0}
        container_name: broker-3
        restart: unless-stopped
        ports:
            - '9094:9094'
            - '19094:19094'
            - '39094:39094'
            - '29094:29094'
            - '9994:9994'
        depends_on:
            - zookeeper
        environment:
            KAFKA_BROKER_ID: 3
            KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
            KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT
            KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
            KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker-3:29094,EXTERNAL://localhost:9094
            # KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker-3:29094,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9094,DOCKER://host.docker.internal:29094
            KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
            KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
            KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
            KAFKA_DEFAULT_REPLICATION_FACTOR: 3
            KAFKA_NUM_PARTITIONS: 3
            KAFKA_JMX_PORT: 19104
            KAFKA_JMX_HOSTNAME: ${PUBLIC_IP:-127.0.0.1}
        volumes:
            - broker_3_data:/var/lib/kafka/data
        healthcheck:
            test: nc -vz localhost 9094
            interval: 10s
            timeout: 10s
            retries: 3
        networks:
            - kafka
        logging:
            driver: "json-file"
            options:
                max-size: "1m"
    
    kafka-ui:
        container_name: kafka-ui
        image: provectuslabs/kafka-ui:latest
        ports:
            - 8087:8080
        depends_on:
            - broker-1
            - broker-2
            - broker-3
        environment:
            KAFKA_CLUSTERS_0_NAME: broker-1
            KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: broker-1:29092
            KAFKA_CLUSTERS_0_METRICS_PORT: 19101
            KAFKA_CLUSTERS_1_NAME: broker-2
            KAFKA_CLUSTERS_1_BOOTSTRAPSERVERS: broker-2:29093
            KAFKA_CLUSTERS_1_METRICS_PORT: 19102
            KAFKA_CLUSTERS_2_NAME: broker-3
            KAFKA_CLUSTERS_2_BOOTSTRAPSERVERS: broker-3:29094
            KAFKA_CLUSTERS_2_METRICS_PORT: 19103
            DYNAMIC_CONFIG_ENABLED: 'true'
        networks:
            - kafka
        logging:
            driver: "json-file"
            options:
                max-size: "1m"
    setup:
        image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
        volumes:
            - certs:/usr/share/elasticsearch/config/certs
        user: "0"
        command: >
            bash -c '
              if [ x${ELASTIC_PASSWORD} == x ]; then
                echo "Set the ELASTIC_PASSWORD environment variable in the .env file";
                exit 1;
              elif [ x${KIBANA_PASSWORD} == x ]; then
                echo "Set the KIBANA_PASSWORD environment variable in the .env file";
                exit 1;
              fi;
              if [ ! -f config/certs/ca.zip ]; then
                echo "Creating CA";
                bin/elasticsearch-certutil ca --silent --pem -out config/certs/ca.zip;
                unzip config/certs/ca.zip -d config/certs;
              fi;
              if [ ! -f config/certs/certs.zip ]; then
                echo "Creating certs";
                echo -ne \
                "instances:\n"\
                "  - name: es01\n"\
                "    dns:\n"\
                "      - es01\n"\
                "      - localhost\n"\
                "    ip:\n"\
                "      - 127.0.0.1\n"\
                "  - name: kibana\n"\
                "    dns:\n"\
                "      - kibana\n"\
                "      - localhost\n"\
                "    ip:\n"\
                "      - 127.0.0.1\n"\
                > config/certs/instances.yml;
                bin/elasticsearch-certutil cert --silent --pem -out config/certs/certs.zip --in config/certs/instances.yml --ca-cert config/certs/ca/ca.crt --ca-key config/certs/ca/ca.key;
                unzip config/certs/certs.zip -d config/certs;
              fi;
              echo "Setting file permissions"
              chown -R root:root config/certs;
              find . -type d -exec chmod 750 \{\} \;;
              find . -type f -exec chmod 640 \{\} \;;
              echo "Waiting for Elasticsearch availability";
              until curl -s --cacert config/certs/ca/ca.crt https://es01:9200 | grep -q "missing authentication credentials"; do sleep 30; done;
              echo "Setting kibana_system password";
              until curl -s -X POST --cacert config/certs/ca/ca.crt -u "elastic:${ELASTIC_PASSWORD}" -H "Content-Type: application/json" https://es01:9200/_security/user/kibana_system/_password -d "{\"password\":\"${KIBANA_PASSWORD}\"}" | grep -q "^{}"; do sleep 10; done;
              echo "All done!";
            '
        healthcheck:
            test: ["CMD-SHELL", "[ -f config/certs/es01/es01.crt ]"]
            interval: 1s
            timeout: 5s
            retries: 120
    
    es01:
        depends_on:
            setup:
                condition: service_healthy
        image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
        container_name: elasticsearch
        labels:
            co.elastic.logs/module: elasticsearch
        volumes:
            - certs:/usr/share/elasticsearch/config/certs
            - esdata01:/usr/share/elasticsearch/data
        ports:
            - 9200:9200
        environment:
            - node.name=es01
            - cluster.name=${CLUSTER_NAME}
            - discovery.type=single-node
            - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
            - bootstrap.memory_lock=true
            - xpack.security.enabled=true
            - xpack.security.http.ssl.enabled=true
            - xpack.security.http.ssl.key=certs/es01/es01.key
            - xpack.security.http.ssl.certificate=certs/es01/es01.crt
            - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
            - xpack.security.transport.ssl.enabled=true
            - xpack.security.transport.ssl.key=certs/es01/es01.key
            - xpack.security.transport.ssl.certificate=certs/es01/es01.crt
            - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
            - xpack.security.transport.ssl.verification_mode=certificate
            - xpack.license.self_generated.type=${LICENSE}
        mem_limit: ${ES_MEM_LIMIT}
        ulimits:
            memlock:
                soft: -1
                hard: -1
        healthcheck:
            test:
              [
                  "CMD-SHELL",
                  "curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'",
              ]
            interval: 10s
            timeout: 10s
            retries: 120
    
    kibana:
        depends_on:
            es01:
                condition: service_healthy
        image: docker.elastic.co/kibana/kibana:${STACK_VERSION}
        container_name: kibana
        labels:
            co.elastic.logs/module: kibana
        volumes:
            - certs:/usr/share/kibana/config/certs
            - kibanadata:/usr/share/kibana/data
        ports:
            - ${KIBANA_PORT}:5601
        environment:
            - SERVERNAME=kibana
            - ELASTICSEARCH_HOSTS=https://es01:9200
            - ELASTICSEARCH_USERNAME=kibana_system
            - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD}
            - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=config/certs/ca/ca.crt
            - XPACK_SECURITY_ENCRYPTIONKEY=${ENCRYPTION_KEY}
            - XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY=${ENCRYPTION_KEY}
            - XPACK_REPORTING_ENCRYPTIONKEY=${ENCRYPTION_KEY}
        mem_limit: ${KB_MEM_LIMIT}
        healthcheck:
            test:
              [
                  "CMD-SHELL",
                  "curl -s -I http://localhost:5601 | grep -q 'HTTP/1.1 302 Found'",
              ]
            interval: 10s
            timeout: 10s
            retries: 120
    
    logstash:
        depends_on:
            es01:
                condition: service_healthy
            kibana:
                condition: service_healthy
        image: docker.elastic.co/logstash/logstash:${STACK_VERSION}
        container_name: logstash
        labels:
            co.elastic.logs/module: logstash
        user: root
        volumes:
            - logstashdata01:/usr/share/logstash/data
            - certs:/usr/share/logstash/certs
            - "./data/logstash_ingest_data/:/usr/share/logstash/ingest_data/"
            - ./elk_stack/logstash/pipeline/logstash.conf:/usr/share/logstash/pipeline/logstash.conf:ro
        environment:
            - NODE_NAME="logstash"
            - xpack.monitoring.enabled=false
            - ELASTIC_USER=elastic
            - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
            - ELASTIC_HOSTS=https://es01:9200
        command: logstash -f /usr/share/logstash/pipeline/logstash.conf
        ports:
            - "5044:5044/udp"
        mem_limit: ${LS_MEM_LIMIT}
        
volumes:
    certs:
        driver: local
    esdata01:
        driver: local
    kibanadata:
        driver: local
    logstashdata01:
        driver: local

networks:
    default:
        name: elastic
        external: false